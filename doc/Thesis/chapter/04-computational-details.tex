\section{Evolutionary Model-based Multiresponse Approach (EMMA)}
\label{sec:ss}
%\todo{for what?}
An evolutionary approach was chosen, namely a multi-objective 
\gls{pso} algorithm called \gls{emma}\cite{villanova2010function,Kennedy1995,Breiman1997,Carta2011}.
%
%"PSO is a population based heuristic inspired by the flocking behavior of birds. 
%To simulate the behavior of a swarm, each bird (or particle) is allowed to fly towards the optimum solution."\cite{villanova2010function}
%
Initially, the input parameters (independent variables), their boundaries and number of equidistant levels for each parameter are declared (see table \ref{tab:input}).
Next, the output variables (dependant variables), their weights in the objective function (the function which should be optimized) are specified and if they should be minimized or maximized is noted.
%
%An initial population of particles, i.e.\ experiments with certain parameters, is chosen out of the population space (space spanned by all possible combinations of input parameters), 
\begin{table}[htb]
	\centering
	\begin{tabular}{cc cc cc}
		\hline\hline
        $c_{zr}$ [\SI{22}{\milli\mol\per\liter}]	&$n_L$	&$T_{C}$[\oc{}]	&$v_{C}$[\mm{}/\s{}]	&$T_{cal}$[\oc{}]	&$v_{cal}$[\oc{}/\h{}]	\\
		\hline
		2				&4		&40					&10				&300				&120	\\
		3				&6		&50					&12				&400				&360	\\
		4				&8		&60					&14				&500				&600	\\
		5				&10		&70					&16				&					&840	\\
						&12		&80					&18				&					&1080	\\
						&		&					&20				&					&		\\
		\hline\hline
	\end{tabular}
	\caption{Discrete levels of each input parameter }
	\label{tab:input}
\end{table}

The \textit{optimizands} where chosen to be $\gamma$, $\rho$, $n_L$ and $v_{cal}$, 
(i.e.\ conductivity, pinhole density, number of coated layers and heating rate)
where $\gamma$, $\rho$ and $n_L$ are to be minimized and $v_{cal}$ to be maximized.
%which all should be minimized except for $v_{cal}$.
The next step is to generate an initial population (ensemble of experiments), which is chosen randomly from the population space. 
The samples are produced, the \gls{iv} curves are measured and the measurements evaluated according to section 
\ref{sec:exp}, yielding the quantities 
$\gamma$, $\rho_s$, $n_L$ and $v_{cal}$, which were used as inputs for the program.
% and the responses conductivity $\gamma$ (see eq. \ref{eq:gamma}), pinhole density $\rho_s$ (see eq. \ref{eq:rho}), $n_L$ (numbers of coating layers) and $v_{cal}$ (heating rate of calcination process in \oc{}/\minutes{}) are supplied to the program. 

The program, then, uses this data to estimate a response for each output variable 
for the entire population space
(and to choose a fraction of the initial population which is allowed to propagate).
%The response variables for the entire population space is calculated. 
The current population - each of the particles independently - 
are given a random velocity vector associated to them, which is directed to the best predicted sample. 
%moves towards the optimum solution.
The population for the next time step is generated and 
again the samples are produced, the \gls{iv} curves are measured and measurements evaluated.
%the experiments are again \td{executed, measured and evaluated. (unclear!)}
Each time step thus includes experiment identification, solution preparation, coating deposition and layer analysis. 

\ds{system behavior are discussed in Modeling the Results.
Algorithm and Coating Features. The theoretical efficiency
of the proposed approach has been verified using a simulation
based on a preliminary experimental model (48 recipes evalu-
ated, see the Supporting Information, S3). Then, 15 new experi-
ments were selected for each subsequent time instant. Each time
instant involves experiment identification, solution preparation,
coating deposition, and spot analysis.
}%\cite{Carta2011}

\section{Fitting via Machine Learning}
%\td{scarce data may lead to overfitting\cite{Lecun1995conv}}\\
Python 3.10.9 and sci-kit learn 1.1.2\cite{pedregosa2011scikit} were used to implement a linear fit model, \gls{krr} and \gls{svm} with polynomial, \gls{rbf} and sigmoid kernels. 
%\subsection{Grid Search}

A grid search over a set of hyperparameters was executed. 
%https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py
%https://scikit-learn.org/stable/modules/svm.html#svc
% KRR 
The hyperparameters which were examined in \gls{krr} were kernel, $\gamma_{ml}$  and $\alpha$ (regularization parameter).
The kernel took the form of \gls{rbf}, sigmoid or polynomial with degree 1--3.
The parameters $\alpha$ and $\gamma_{ml}$ (subscript to distinguish from \textit{optimizand}) took the values 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5 and 10.
Note that $\gamma_{ml}$ is used differently in each kernel. 

% SVM
The kernels for \gls{svm} were the same as for \gls{krr}. 
\Gls{svm} uses $C$ instead of $\alpha$ as regularization parameter) and  $\varepsilon$ as \gls{svm} tube size parameter.
$C$ and $\varepsilon$ took the values 0.05--1 with step size 0.05. 
\Gls{svm} additionally used the \texttt{scale} for $\gamma_{ml}$.
\texttt{scale} equals to $\frac{1}{d \cdot \sigma(\mathbf{X})}$ with $d$ being the number of features 
and $\sigma(\mathbf{X})$ the variance of the input variables. 

% GRID SEARCH 
The input variables ($c_{zr}$, $n_L$, $v_{C}$, $T_{C}$, $v_{cal}$, $T_{cal}$) 
of the \gls{emma} data set were scaled with the routine \texttt{sklearn.preprocessing.StandardScaler().fit\_transform()}. %\td{scaled before each calculation.}
% https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge.score
% https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR.score
For each calculation the 5-fold cross-validation score was calculated. 
The score equals to the coefficient of determination $R^2$, which is defined the following for non-linear regression functions:
%\footnote{see https://scikit-learn.org/stable/modules/generated/sklearn.kernel\_ridge.KernelRidge.html\#sklearn.kernel\_ridge.KernelRidge.score}
\footnote{see \href{https://scikit-learn.org/stable/modules/generated/sklearn.kernel\_ridge.KernelRidge.html}{https://scikit-learn.org/stable/modules/generated/sklearn.kernel\_ridge.KernelRidge.html}}
\begin{equation}
	\label{eq:r2}
	R^2 = 1-\frac{\sum (y_i - \hat{y}_i)}{\sum (y_i - \bar{y}_i)}
\end{equation}
For each method (\gls{krr} and \gls{svm}) and each \textit{optimizand} ($\gamma$ and $\rho$) 
the parameters with the highest score were noted.
 
Finally, the different data sets were predicted for each model and \gls{mae} and \gls{mse} were calculated.
These values were also calculated for \gls{emma} and the linear model in order to compare the different methods. 
%\todo{(and how did this output help you to optimize your exp. procedure??)}
