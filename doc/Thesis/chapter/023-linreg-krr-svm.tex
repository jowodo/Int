%
\textbf{Linear regression} is one of the simplest methods to predict data. 
It persuades by its computational simplicity and easy interpretation. 
%Let's take this equation
\begin{equation}
\mathbf{y} = \mathbf{X} \mathbf{k} +c 
\end{equation}
Where $\mathbf{y} \in \mathbb{R}^n$ ($n$ is the number of data points) is a vector of dependent variables which shall be predicted and $\mathbf{X}\in \mathbb{R}^{n\times d}$ ($d$ is the number of independent variables) is a matrix of independent variables. 
The parameters $\mathbf{k} \in \mathbb{R}^d$ and $c \in \mathbb{R}$ are chosen by minimizing an objective function (loss function in \gls{ml} jargon).
A typical function to minimize are the $L_1$ and $L_2$ error, i.e.\ \gls{mae} and \gls{mse}, respectively.
\begin{align}
L_1&= \sum_i |y_i - \hat y_i| \\%= \sum_i \sum_j |y_i - x_{i,j}\cdot k_i+c| \\
L_2&= \sum_i(y_i - \hat y_i)^2 %= \sum_i \sum_j (y_i - x_{i,j}\cdot k_i+c)^2
\end{align}
%

\textbf{Ridge Regression} is like linear regression with an extra term which penalizes steep regression functions.
The extra term reduces overfitting and is scaled by a correction parameter~$\alpha$. 
The larger $\alpha$ is, the larger is the regularization and the flatter is the regression function. 
When $\alpha$ tends to infinity, we get an intercept-only model.
When $\alpha$ is zero, the regularized loss function becomes the $L_2$ loss function.
%\td{A critique of ridge regression is that all the variables tend to end up in the model. The model only shrinks the coefficients.}
\begin{equation}
    L_{RR} = L_2 + \alpha \sum |k_i| = \sum(y_i - \hat y_i)^2 + \alpha \sum |k_i| 
\end{equation}

%\textbf{Kernel Ridge Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%% { Kernel Ridge Regression } KRR %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{\Gls{krr}} combines ridge regression with the kernel method. 
A~kernel transforms data in such a way that a linear hyperplane (a point in one dimension, a line in two dimensions, 
a plane in three dimensions) can fit data in regression problems or separate the data in classification problems without actually doing the transformation for every data point, which lowers computational costs.
A kernel is some kind of similarity measure 
which fulfills the requirements of non-negativity, symmetry and linearity\cite{rupp2015machine}.
%\enlargethispage{\baselineskip}
%\looseness=-1
The following equations show definitions for 
linear~(eq.~\ref{eq:lin-kernel}), 
polynomial (eq.~\ref{eq:pol-kernel}), 
sigmoidal (eq.~\ref{eq:sig-kernel}) and 
radial basis functional kernel~(eq.~\ref{eq:rbf-kernel}), with $\gamma$ as fixed hyperparameter and $c_0$ as parameter to optimize.
A requirement for using kernels is having a dot product in the loss function. 
This can be accomplished by expressing $\mathbf{k}$ in terms of $\mathbf{X}$: $\mathbf{k}=\mathbf{X}^\top \mathbf{r}$
\cite{rudin2020least}.

\begin{align}
    \label{eq:lin-kernel}
    k_{lin}(\mathbf{x},\mathbf{y}) &= \mathbf{x}^{\top} \mathbf{y} \\
    \label{eq:pol-kernel}
    k_{pol}(\mathbf{x}, \mathbf{y}) &= (\gamma \mathbf{x}^{\top} \mathbf{y} + c_0)^d \\
    \label{eq:sig-kernel}
    k_{sig}(\mathbf{x}, \mathbf{y}) &= \text{tanh}(\gamma \mathbf{x}^{\top} \mathbf{y} + c_0) \\
    \label{eq:rbf-kernel}
    k_{rbf}(\mathbf{x}, \mathbf{y}) &= \text{exp}(- \gamma \| \mathbf{x} -  \mathbf{y} \|^2) 
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%% { Support vector machines } SVM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{\Gls{svm}} is a versatile machine learning algorithm first mentioned in 1992\cite{boser1992training}. 
%
The \gls{svm} was initially developed by Vladimir Vapnik for the binary classification of separable data, then improved to handle non-separable data\cite{cortes1995support} and eventually adapted to solve regression problems\cite{drucker1996support}.
The concepts of \gls{svm} will be discussed in the same chronological order. 
%
Classification works by spanning a hyperplane between two linearly separable categories in a way such that the closest points from each category have the largest distance to the hyperplane. 
The distance from the points to the hyperplane is called margin $\tau$.
The points with the shortest distance to the hyperplane are called support vectors and are used to define the hyperplane.
A \gls{svm} avoids overfitting by only using a subset of the data - the support vectors - to fit the model. 
The goal is to find the decision boundary which correctly classifies all samples with the biggest margin. 
%
The decision boundary can be expressed as a hyperplane
\begin{equation}
	\hat{\mathbf{y}} = h(\mathbf{X}) = \mathbf{X} \cdot \mathbf{w} + b
\end{equation}
with 
$\mathbf{y} \in \lbrace +1, -1 \rbrace^n$, 
$\hat{\mathbf{y}} \in \mathbb{R}^n$,
$\mathbf{X} \in \mathbb{R}^{n \times d}$ and
$\mathbf{w} \in \mathbb{R}^d$.
The constraint of the positive and negative support vectors ($\mathbf{x}^+$ and $\mathbf{x}^-$, respectively) satisfying
\begin{align}
	\mathbf{x}^+ \cdot \mathbf{w} + b &= 1 \\
	\mathbf{x}^- \cdot \mathbf{w} + b &= -1 
\end{align}
can be generalized to 
\begin{equation}
	\label{eq:svm01}
	y_i(\mathbf{x}_i \cdot \mathbf{w} + b ) \geqslant 1 %\qquad i=1,\dots,n
\end{equation}
where $y_i$ being the labels of the training data. 
The width of the margin can be inferred by projecting the vector spanning between two support vectors 
on opposite sides of the decision boundary onto the unit vector perpendicular to the hyperplane. 
If we now take a vector from a positive support vector $\mathbf{x}^+$ to a negative support vector $\mathbf{x}^-$ and project it onto the unit vector of $\mathbf{w}$ (which is perpendicular to the hyperplane), 
we get the width of the margin. 
\iffalse
\begin{equation}
	\label{eq:svm02}
	\begin{aligned} 
		2\tau &= (\mathbf{x}^+ - \mathbf{x}^-) \cdot \frac{\mathbf{w}}{\|\mathbf{w}\|} \\
			&= (x^+ \cdot \mathbf{w} - x^-\cdot \mathbf{w} ) \cdot \frac{1}{\|\mathbf{w}\|} \\
			&= ( (1-b) - (-1-b) ) \cdot \frac{1}{\|\mathbf{w}\|} \\
			&= \frac{2}{\|w\|}
	\end{aligned}
\end{equation}
\fi
\begin{equation}
	\label{eq:svm02}
	\begin{aligned} 
		2\tau &= (\mathbf{x}^+ - \mathbf{x}^-) \cdot \frac{\mathbf{w}}{\|\mathbf{w}\|} 
			= (\mathbf{x}^+ \cdot \mathbf{w} - \mathbf{x}^-\cdot \mathbf{w} ) \cdot \frac{1}{\|\mathbf{w}\|} \\
			&= ( (1-b) - (-1-b) ) \cdot \frac{1}{\|\mathbf{w}\|} 
			= \frac{2}{\|\mathbf{w}\|}
	\end{aligned}
\end{equation}
%\td{draw sketch and make \ref{eq:svm02} on two lines}
Thus, maximizing the margin is equivalent with minimizing $\|\mathbf{w}\|$ and minimizing $\frac{1}{2}\|\mathbf{w}\|^2$ (mathematical convenience for further steps).
By incorporation the constraint (eq. \ref{eq:svm01}) via the Lagrangian multiplier method we 
\textit{mathemagically} arrive at the loss function which should be maximized: 
\begin{equation}
	\label{eq:svm03}
	\mathfrak{L} = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_i^n \alpha_i [ y_i ( w_i \cdot x_i + b) -1 ] 
\end{equation}
which can be rewritten by some mathematical 
%wizardry
acrobatics
(setting the partial derivatives of the Lagrangian function
$\frac{\partial \mathfrak{L}}{\partial \mathbf{w}}$ and $\frac{\partial \mathfrak{L}}{\partial b}$ 
to zero and inserting into eq. (\ref{eq:svm03}))\cite{winston1992artificial,cherkassky1998learning} in the following way: 
\begin{equation}
	\label{eq:svm04}
	\mathfrak{L} = \sum_i^n \alpha_i - \frac{1}{2} \sum_i^n \sum_j^n \alpha_i \alpha_j y_i y_j \mathbf{x}_i \cdot \mathbf{x}_j 
\end{equation}
%
%%%%%%%%%% SOFT MARGIN SVM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Soft margin \gls{svm}} -- in contrast to hard margin \gls{svm} -- is used if the data is non-separable due to outliers\cite{cortes1995support}.
Such data can be handled by introducing a penalization term for wrongly categorized samples into the loss function. 
\begin{equation}
	\label{eq:svm-soft}
	L_{SM} = \frac{1}{2} \|\mathbf{w}\|^2 + \sum_i^n max \left( 0, 1- y_i ( \mathbf{w} \mathbf{x}_i + b ) \right) 
\end{equation}
If the prediction $(\mathbf{w}\mathbf{x}_i+b)$ and the true category $y_i$ do not agree, 
they have opposite signs and their product will be a negative number.
The subtraction of a negative number will result in a positive penalization. 
If the sample is correctly predicted, the product will result in a positive number, 
the substraction will lead to a negative number and the maximum will be 0 if the sample is outside of the margin. 
%
%%%%%%%%%%% KERNELS IN SVM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%A big advantage of \gls{svm} is that through the kernel trick non-linearly separable data can be divided. 
%As mentioned before, a inner product is need to use the kernel trick. 
The function we want to optimize (eq.~\ref{eq:svm04}) and its soft margin equivalent (eq.~\ref{eq:svm-soft}) can both be expressed as inner products. 
This allows us to separate data not only by hyperplanes 
but also by intricate decision boundaries due to the kernel trick. 

Again, a kernel $K(\mathbf{x}_i, \mathbf{x}_j) = T(\mathbf{x}_i)^\top \cdot T(\mathbf{x}_j)$ allows to calculate the inner product of two vectors in a transformed space without the need of transforming each vector, which turns out to be computationally much cheaper. 
%\td{write about sv regrgession}

\Gls{svm}s can also be used for regression (support vector regression).
The decision function becomes the regression function and the margin includes all data points instead of none. 
In soft margin support vector regression a few
outliers are allowed.
%data point can lie outside the margin as outliers. 
Non-linearity can be likewise introduced via the kernel trick. 
%\td{write about kernel} 

