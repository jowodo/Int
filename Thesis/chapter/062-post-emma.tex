\label{sec:res-post-emma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{ANOVA}\label{sec:res-anova}
\textbf{EMMA}
The \gls{rf} produced by the last iteration of \gls{emma} put a high \td{significance} 
on the calcination temperature $T_{cal}$. 
Now \gls{anova} can be used to check if the data 
From emma we saw that TCal seems to hav the biggest influence on 
both gamma and rho. 

\textbf{What can/did we test?}
With anova we can check if the probability of arriving at these results was by pure chance or not. 
With the null hypothesis that we arrive at results by pure chance and of that the probability. 
We tested for each input variable the influence on rho and gamma 
and also as a test on vcal and $\lambda$. 
We used an alpha value of 0.05 which means that 1 out of 20 times this result could be 
produced by pure chance. 
If the f-value is below that value, then we choose to accept, that the data is not due to
chance but due to the influence of the independent variables on the dependent variables. 
Usually, the alpha-value is choosen to be 0.01-0.05\cite{hoffman2020concept,sellke2001pvalues}
The choice of a relatively high alpha-value is gerechetfertigt by the rather small amount of data and \td{evn}.
\td{could test anova on difference generation data sets} 

\textbf{What were the results?}
\td{anova shows that for the mentioned indep and dep pairs, the means divieded by 
indep groups are not equal and that there is at least one group that is different. 
Important to mention, that anova uses categorical data for inputs, 
where our data is numerical and thus information is lost}
For both gamma and lambda (for both the emma dataset and the extended dataset) 
the F values for Tcal were in all cases under 0.01, 
indicating a real influence of Tcal on the conductance.
Both datasets exhibit a $p(>F) < 0.05$ for the interaction Tcal:vcal on phd. 
\td{why tcal:vcal on phd the same for both datasets?}
And the extended data set has pF samll er than 0.01 of vcal on phd. 

\textbf{What do the results mean?} 
Anova bestaetigt that TCal has influence on both G and phd. 
\td{how to interpret p-values of 0.01 and 0.05? 
They mean that under the assumption that the null hypothesis is correct, 
there is a 1\% and 5\% chance of arriving at such data as observed.}
It seems that the probability of getting such data per chance that Tcal has no influence 
(null hypothesis is true) is less then 5\% (even less than 1\%). \td{NO bcs \cite{sellke2001pvalues}}

%\textbf{Pre-writing:}
%\Gls{anova} doesn't provide a lot of extra information. 
%wrong the second is not for phd but for extended data set
%but for phd even lower than 0.1 \%
%Anova shows that the influence of TCal is significant as shown by the F 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{Linear Regression}
I thought that the influnce of $conc$ and $\lambda$ are the biggest, but they are just the smalles absolute values. 
Thus, eventhough $T_{cal}$ might have a smaller coefficient for lin reg, the influence on the dependent variable might be larger. 
The biggest influece on $\rho$ is indeed $T_{cal}$, next with about a third of the influence $T_{doc}$. 
Both $T_{cal}$ and $T_{doc}$ have positive coefficients, i.e. a negative influence on the resistance. 
Then both $\lambda$ and $v_{doc}$ both have about a fift ob the largest influence and a negative influence. 
An indication that the data is strongly tainted by error (see also section \ref{sec:res-anova}) is the low $R^2$ score of 0.41, 
which is (sad but true) even unterboten by the $R^2$ of lin fit of $G$, 0.34. 
The coefficients, though share the signs. 
This is only true if the extended data set is used and therefore again rather by chance. 
It is even more \td{erstaunlich} that EMMA managed to decrease the average of $pG$ an $\rho$ with each generation (see figure \ref{fig:emma-gen}. 
Or was is only luck? 

$v_{cal}$ and $\lambda$ are well predicted by linear regression as there is a perfect direct proportionality. 
Was zu erwarten war. 


Do multi linear reg like in Peprah et al. 2017\cite{peprah2017appraisal}.

%MSE(data=all) =427
%MSE(data=emma)=250

%\iffalse
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{Grid Search}
For \gls{krr} and {svm} I had to find out what the ideal hyper parameters were. 
So, I anstellen a grid search with the following hyperparameters: 
%kernel=["poly","rbf"]#,"sigmoid"]
%C= np.array(range(1,20))*0.05
%degree=range(1,6)
%epsilon= np.array(range(1,20))*0.2
%gamma=[0.0,0.1,1.0,10.0]
%#param = {"kernel":kernel, "C":C, "degree":degree, "epsilon":epsilon, "gamma":gamma }
%C=[1,0.1]; degree=[3]; epsilon=[1,2]; gamma=['scale']
Now what are C,epsilon and gamma? 

Since the data set was very small, I could be generous with the grid search even on relatively humble hardware (intel i7-8550U). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{KRR}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{SVM}
\td{plot the predicted data for variables which should be excluded (Tcal, Vcal,Conc, layers)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{PCR?}

%\fi

\subsubsection{Further}
\begin{itemize}
    \item grid search
    \item KRR (start with grid search) (???)
    \item SVM (start with grid search) (???)
    \item \url{https://blog.minitab.com/en/adventures-in-statistics-2/regression-smackdown-stepwise-versus-best-subsets}
    \item \url{https://blog.minitab.com/en/how-to-choose-the-best-regression-model}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{Comparison} 
compare all methods. 
How should I compare them? 
What are the measurables. 

