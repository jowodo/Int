\subsection{Post-EMMA}
\label{sec:res-post-emma}
%describe what awaits the reader in this chapter:
In this section the experimental results are analyzed with different methods and compared with the original method EMMA which was integrated into the experiment selection process. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{ANOVA}\label{sec:res-anova}
%\textbf{EMMA}
The \gls{rf}s produced by the last iteration of \gls{emma} (see equations \ref{eq:emma-phd4} and \ref{eq:emma-G4}) put a high significance
%/effect/correlation
on the calcination temperature $T_{cal}$ with regard to the 
measures of the conductance. 
\Gls{anova} is used to double check this. 
%Now \gls{anova} can be used to check if the data 
%\td{ANOVA was used to check if \gls{bf} identified by \gls{emma} are reasonable.} 

\textbf{What can/did we test?}
the probability of arriving at these results was by pure chance can be checked with \gls{anova}. 
The null hypothesis is that the results were obtained by pure chance without any dependence of the insulation of a sample on the process variables.
%We tested for each input variable the influence on rho and gamma 
A one-way \gls{anova} was performed for every independent variable on $\rho$ and $\gamma$. 
%and also as a test on vcal and $\lambda$. 
We used an $\alpha$ value of 0.05 which means that 
the null hypothesis is rejected if the p-value is under 0.05. 
Usually, the $\alpha$-value is chosen to be 0.01 or 0.05\cite{hoffman2020concept,sellke2001pvalues}.
The choice of a relatively high $alpha$-value is motivated by the rather small amount of data and low events per varible ratio.
\ds{
As mentioned by Sellke et al.\cite{sellke2001pvalues} a p-value of 0.05 might tempt 
to guess that in 1 out of 20 cases the results could be false positives, 
whereas the frequency is much higher. 
A p-value of 0.01 and 0.05  mean that under the assumption that the null hypothesis is correct, 
there is a 1\% and 5\% chance of arriving at such data as observed.
%The results should thus be taken with a grain of salt. 
}
%1 out of 20 times this result could be 
%produced by pure chance given that the null hypothesis is true. 
%If the p-value is below that value, then we choose to accept, that the data is not due to
%chance but due to the influence of the independent variables on the dependent variables. 
%\td{could test anova on different generations data sets} 

\textbf{What were the results?}
%\Gls{anova} shows that for the mentioned indep and dep pairs, the means divided by 
%indep groups are not equal and that there is at least one group that is different. 
For both $\rho$ and $\lambda$ (for both the \gls{emma} dataset and the extended dataset (=\gls{emma}+pre\gls{emma})) 
the F-values for $T_{cal}$ were in all cases under 0.01, 
indicating a real influence of $T_{cal}$ on the conductance.
Furthermore, both datasets exhibit a p-value $< 0.05$ for the interaction $T_{cal}$:$v_{cal}$ on $\rho$
and the extended data set has p-value smaller than 0.01 of $v_{cal}$ on $\rho$. 
Whereas $v_{cal}$ on $\rho$ for \gls{emma} dataset has p-value of slightly over 0.05
%\td{why tcal:vcal on phd the same for both datasets? zufall}

\textbf{What do the results mean?} 
\Gls{anova} confirms that $T_{cal}$ has influence on both $\gamma$ and $\rho$. 
It seems that the probability of getting such data per chance that Tcal has no influence 
(null hypothesis is true) is less then 5\% (even less than 1\%). 
%\td{NO bcs \cite{sellke2001pvalues}}
It is important to mention, that \gls{anova} assumes categorical data for inputs, 
where our data is numerical and thus information is lost. 

%\textbf{Pre-writing:}
%\Gls{anova} doesn't provide a lot of extra information. 
%wrong the second is not for phd but for extended data set
%but for phd even lower than 0.1 \%
%Anova shows that the influence of TCal is significant as shown by the F 

%%%%%%%%%%%%%%%%%%%% LIN REG %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{Linear Regression} 
%Initially, it was thought that the resulting layer and thus resistance of a sample is mainly influennced by the Zirconium concentrain in the starting solution $c_{zr}$ and the number of layers applied $\lambda$, but their coefficients are the smallest in absolute values. 
The coefficients of
the Zirconium concentration in the starting solution $c_{zr}$ and the number of layers applied $\lambda$ are largest in value, but the values which are taken by $c_{zr}$ and $\lambda$ are the smallest among the input variables. 
%Initially, it was thought that the resulting layer and thus resistance of a sample is mainly influennced by
%but their coefficients are the smallest in absolute values. 
%
\begin{align}
	\begin{split}
		\label{eq:linreg-phd}
		\hat{\rho} =&  -0.022\cdot c_{zr} -0.022\cdot \lambda -0.0029\cdot v_{DB} + 0.0058\cdot T_{DB} \\
		& -8.5\cdot 10^{-5}\cdot v_{cal} + 0.0026\cdot T_{cal} -0.76
	\end{split}
	\\
	\begin{split}
		\label{eq:linreg-G}
		\hat{\gamma} ={} & 3.2\cdot c_{zr} - 0.97\cdot \lambda - 1.2\cdot v_{DB} + 0.2\cdot T_{DB} \\
			& - 0.014\cdot v_{cal} + 0.13\cdot T_{cal} + 7.2
	\end{split}
\end{align}
%
All terms have the same signs in equations \ref{eq:linreg-phd} and \ref{eq:linreg-G} have the same sign except the intercept and $c_{zr}$. 
$T_{cal}$ and $T_{DB}$ have positive coefficients, i.e. a negative influence on the resistance 
and $\lambda$,$v_{DB}$ and $v_{cal}$ have negative coefficients, i.e. a positive influence on resistance. 
An indication that the data is strongly tainted by error (see also section \ref{sec:res-anova}) is the low $R^2$ score of 0.41 of $\hat\rho$, 
which is (sad but true) even undercut by the $R^2$ of linear fit of $\hat\gamma$, 0.34. 
%The coefficients, though share the signs. 
%This is only true if the extended data set is used and therefore again rather by chance. 
When the linear regression is trained on the complete data set, all terms have the same sign. 
%It is even more astounding that \gls{emma} managed to decrease the average of $\gamma$ an $\rho$ with each generation (see figure \ref{fig:emma-gen}. 
%
\begin{table}[htb]
	\center
	\begin{tabular}{cccccccc}
		        & $c_{zr}$      & $\lambda$     & $v_{DB}$      & $T_{DB}$      & $v_{cal}$     & $T_{cal}$\\
				\hline\hline
				$\rho$          &2.04   &46.58  &1.15   &9.21   &13.50  &27.52\\
				$\gamma$        &10.24  &7.10   &16.46  &10.97  &7.68   &47.55\\
				\hline\hline
	\end{tabular}
	\label{tab:lin-reg-influence}
	\caption{Influences of input variables on output variables in percent according to linear regression.}
\end{table}
%
Thus, even though $T_{cal}$ might have a smaller coefficient in the linear \gls{rf}, the influence on the dependent variable might be larger. 
The influence was calculated by the formula: $I_i=\frac{\bar{x}_i \cdot k_i}{\sum_i \bar{x}_i \cdot k_i}$ with $i=1,\dots,d $ and where $\bar{x}_i$ is average value of the $i$-th input variable and $k_i$ is the coefficient of the $i$-th input variable in the linear regression. 
%The biggest influence on $\rho$ is indeed $T_{cal}$, next with about a third of the influence $T_{DB}$. 
The biggest influence on $\hat\rho$ and $\hat\gamma$ are $\lambda$ and $T_{cal}$. 
While $T_{cal}$ is the second largest influence on $\hat\rho$ as well it is noteworthy that $\lambda$ contributes the least to $\hat\gamma$. 
Furthermore the influence on $\hat\rho$ is dominated by $\lambda$ and $T_{cal}$, while the influences on $\hat\gamma$ are distributed more evenly.
Different distributions were not expected. 
This can have two reasons: 
either the *optimizands* darstellen differen properties and/or signal to noise ratio is too low.
%

$v_{cal}$ and $\lambda$ are well predicted by linear regression as there is a perfect direct proportionality. 
%Do multi linear reg like in Peprah et al. 2017\cite{peprah2017appraisal}.
%

%MSE(data=all) =427
%MSE(data=emma)=250

%\iffalse
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{Grid Search}
For \gls{krr} and {svm} I had to find out what the ideal hyper parameters were. 
So, I anstellen a grid search with the following hyperparameters: 
%kernel=["poly","rbf"]#,"sigmoid"]
%C= np.array(range(1,20))*0.05
%degree=range(1,6)
%epsilon= np.array(range(1,20))*0.2
%gamma=[0.0,0.1,1.0,10.0]
%#param = {"kernel":kernel, "C":C, "degree":degree, "epsilon":epsilon, "gamma":gamma }
%C=[1,0.1]; degree=[3]; epsilon=[1,2]; gamma=['scale']
Now what are C,epsilon and gamma? 

Since the data set was very small, I could be generous with the grid search even on relatively humble hardware (intel i7-8550U). 

\begin{table}[htb]
	\centering
    \caption{Post EMMA vergleich}
	\label{tab:post-emma}
	\begin{tabular}{c cc cc cc cc}
    \hline\hline
    G&  MAEe&   MSEe&   MAEp&   MSEp&   MAEc&   MSEc&   MAEa& MSEa \\
    \hline
        MARS&   10& 171&    28& 1084&   30& 1280&   17& 548\\
        lin Reg&    13& 250&    24& 747&    34& 1327&   17& 454\\
        ML KRR& 15& 415&    26& 1048&   28& 1586&   19& 676\\
        ML SVM& 15  415&    26& 1050&   28& 1588&   19& 677\\
    \hline\hline
	\end{tabular}
    %
	\begin{tabular}{c cc cc cc cc}
    \hline\hline
    p&  MAEe&   MSEe&   MAEp&   MSEp&   MAEc&   MSEc&   MAEa& MSEa \\
    \hline
        MARS&   0.14&   0.03&   0.41&   0.21&   0.39&   0.18&   0.25&   0.11\\
        lin Reg&    0.19&   0.06&   0.30&   0.15&   0.38&   0.13&   0.24&   0.09\\
        ML KRR& 0.30&   0.22&   0.47&   0.38&   0.52&   0.44&   0.34&   0.26\\
        ML SVM& 0.28&   0.12&   0.37&   0.21&   0.42&   0.25&   0.32&   0.16\\
    \hline\hline
	\end{tabular}
\end{table}
    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{KRR}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{SVM}
\td{plot the predicted data for variables which should be excluded (Tcal, Vcal,Conc, layers)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{PCR?}

%\fi

\subsubsection{Further}
\begin{itemize}
    \item grid search
    \item KRR (start with grid search) (???)
    \item SVM (start with grid search) (???)
    \item \url{https://blog.minitab.com/en/adventures-in-statistics-2/regression-smackdown-stepwise-versus-best-subsets}
    \item \url{https://blog.minitab.com/en/how-to-choose-the-best-regression-model}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{Comparison} 
compare all methods. 
How should I compare them? 
What are the measurables. 


\td{do regression for avg(-log(G)) instead of $\gamma$=avg( ( 13+log(G))**2)} 
