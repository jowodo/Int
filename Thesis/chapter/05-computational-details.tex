\subsection{Data Processing}
\label{sec:eval}
Reading list
\begin{itemize}
    %\item https://de.wikipedia.org/wiki/Globaler_F-Test
    %\item https://de.wikipedia.org/wiki/Optimale_Versuchsplanung
    %\item Curse of dimensionality scholar
    %\item linear regression von ungar 
    %\item ridge regression + support vector machine
    %\item correlation vs covariance
    %\item https://en.wikipedia.org/wiki/Regression_analysis
    %\item https://en.wikipedia.org/wiki/Noise_reduction
    %\item https://en.wikipedia.org/wiki/Feature_extraction
    %\item https://en.wikipedia.org/wiki/Feature_selection
    %\item https://en.wikipedia.org/wiki/Dimensionality_reduction
    %\item file:///home/pur/Doc/Uni/Chemie_Master/Int_AIT/Int.git/Code/ML/98_things_can_go_wrong_in_ML_project.html
    %\item https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
    %\item https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA
    \item 
    \item 
    \item 
\end{itemize}
%\td{describe problem and solution found}
For every sample multiple \gls{iv} curves were measured. 
The main difficulty faced in processing the data was to present the measurements obtain 
from a sample in one representative number. 
The average of each conductance would be an obvious choice but difficult to represent a 
sample correctly since the possible values of conductance span accross several magnitudes.
So the average of logarithms of conductances is the next nearby ansatz.
\todo{why didn't i just take this?}
\td{L2 norm because it might be easier to compare to other methods. }
\td{inspired by euclidean norm/L2 distance from ideal case} 
\td{wanted to put more weight on bad shunts}
\td{plot avg, avg(log), avg(log-13)}

For every I-V curve (aluminium dot) the gradient $g$ at $V=0$ is calculated by taking 
each $n=5$ points after and before the origin at least $d=2$ points distance (which boils
down to the values from \SI{0.02}{\volt} to \SI{0.07}{\volt}) , averaging their V and I values 
and calculating
\begin{equation}
    g = \frac{I_{+m+d} - I_n}{V_{n+1} - V_n}.
\end{equation}
As a measure of conductance a distance D from an ideal non-conducting case. The average of the negative base 10 logarithm subtracted from an ideal non-conducting gradient of $10^{-13}$ 
\begin{equation}
	D = \sum_i^N \frac{ -log_{10}(g_i) - 13}{N}
	\label{eq:D}
\end{equation}
Another measure is the density of shorted species $\rho_{s}$ is calculated in following way:
\begin{equation}
	s_i = \begin{cases}
	1 &\text{if} \quad -log(g_i) < 5 \\
	0 &\text{if} \quad -log(g_i) \geq 5 \\
	\end{cases}
\end{equation}
\begin{equation}
	\rho_s = \sum_i^N \frac{s_i}{N}
	\label{eq:rho}
\end{equation}
Other estimates of the conductance are the averages:
\begin{equation}
	G_1 = log \left( \sum_i^N \frac{g_i}{N} \right)
\end{equation}

\begin{equation}
	G_2 =  \sum_i^N \frac{log(g_i)}{N}
\end{equation}

\subsection{Sample Selection}
\label{sec:ss}
An evolutionary approach was chosen, namely a multi-objective Particle Swarm Optimization (PSO) with a multi-response
Multivariate Adaptive Regression Splines (MARS) model\cite{Villanova2010,Kennedy1995,Breiman1997,Carta2011}.
%
"PSO is a population based heuristic inspired by the flocking behavior of birds. 
To simulate the behavior of a swarm, each bird (or particle) is allowed to fly towards the optimum solution."\cite{Villanova2010}
%
Initially the input parameters (independent variables), their boundaries and number of equidistant levels for each parameter are declared (see table \ref{tab:input}).
Next, the output variables (dependant variables), their weights in the objective function (the function which should be optimized) are specified and if they should be minimized or maximized is noted.
%
%An initial population of particles, i.e. experiments with certain parameters, is chosen out of the population space (space spanned by all possible combinations of input parameters), 
\begin{table}[htb]
	\centering
	\begin{tabular}{cc cc cc}
		\hline
		Zr(PrO)$_4$ conc. [21 g/L]	&layers	&$T_{DB}$[\oc{}]	&$v_{DB}$[\mm{}/\s{}]	&$T_{cal}$[\oc{}]	&$v_{cal}$[\oc{}/\h{}]	\\
		\hline
		2				&4		&40					&10				&300				&120	\\
		3				&6		&50					&12				&400				&360	\\
		4				&8		&60					&14				&500				&600	\\
		5				&10		&70					&16				&					&840	\\
						&12		&80					&18				&					&1080	\\
						&		&					&20				&					&		\\
		\hline
	\end{tabular}
	\caption{Discrete levels of each input parameter \td{are concentrations correct?}}
	\label{tab:input}
\end{table}

The first step is to select an initial population (ensemble of experiments), which is chosen randomly from the population space. 
The samples are made, measured and evaluated according to section \ref{sec:exp} and the distance $D$ (see eq. \ref{eq:D}), $\rho_s$ (see eq. \ref{eq:rho}), $n_{layers}$ (numbers of layers) and $v_{cal}$ (heating rate of calcination process in \oc{}/\minutes{}) are supplied to the program. 
The program uses this data to estimate a response for each output variable (and to choose a fraction of the initial population which is allowed to propagate).
The response variables for the entire population space is calculated. 
The current population - each of the particles independently - moves towards the optimum solution.
The population for the next time step is outputted and the experiments are again executed, measured and evaluated.

\td{system behavior are discussed in Modeling the Results.
Algorithm and Coating Features. The theoretical efficiency
of the proposed approach has been verified using a simulation
based on a preliminary experimental model (48 recipes evalu-
ated, see the Supporting Information, S3). Then, 15 new experi-
ments were selected for each subsequent time instant. Each time
instant involves experiment identification, solution preparation,
coating deposition, and spot analysis.
Figure 2a presents the laser scanner measurement of a spot
}\cite{Carta2011}
%\subsection{EMMA Propagation}
%\begin{table}[htb]
%	\centering
%	\begin{tabular}{ccccccc}
%		\hline
%		\hline
%conc	&layers	&vDOC	&TDOC	&vCal	&Tcal	&	\\
%		\hline
%1	&10	&5	&20	&120	&400	&\\
%1	&4	&0.1	&20	&120	&500	&\\
%5	&10	&0.1	&20	&120	&500	&\\
%5	&4	&5	&20	&480	&400	&\\
%1	&5	&5	&80	&120	&500	&\\
%1	&10	&1	&80	&480	&400	&\\
%5	&5	&1	&80	&120	&400	&\\
%5	&10	&5	&80	&480	&500	&\\
%2	&8	&0.5	&40	&360	&470	&\\
%2	&6	&2	&40	&360	&430	&\\
%		\hline
%%4	&12	&-	&-	&-	&-&\\
%1	&4	&12	&70	&120	&500	&\\
%1	&9	&18	&80	&240	&400	&\\
%%2	&5	&18	&70	&-	&-		&\\
%4	&6	&14	&60	&240	&500	&\\
%4	&6	&14	&60	&240	&500	&\\
%4	&6	&14	&60	&240	&500	&\\
%		\hline
%2	&10	&20	&40	&120	&500	&6113\\
%3	&8	&18	&70	&1080	&300	&2850\\
%3	&6	&10	&50	&1080	&400	&5526	\\
%%3	&10	&14	&50	&600	&500	&-7374	\\
%3	&10	&16	&80	&120	&500	&6554	\\
%4	&6	&16	&80	&1080	&300	&2947	\\
%3	&12	&12	&80	&840	&500	&8318	\\
%3	&10	&14	&50	&600	&500	&7374	\\
%5	&6	&10	&60	&1080	&400	&5648	\\
%5	&10	&20	&60	&360	&300	&3956	\\
%5	&12	&14	&60	&1080	&300	&2700	\\
%		\hline
%%2	&2	&10	&40	&600	&300	&-7201	\\
%2	&4	&10	&80	&1080	&300	&6101	\\
%%2	&4	&10	&40	&600	&300	&-7201	\\
%2	&4	&10	&40	&600	&300	&7201	\\
%3	&4	&12	&60	&600	&300	&1462	\\
%4	&4	&10	&80	&1080	&300	&2883	\\
%5	&12	&20	&70	&600	&300	&1680	\\
%		\hline
%2	&4	&10	&40	&120	&300	&1	\\
%2	&4	&10	&40	&120	&500	&6001	\\
%3	&4	&10	&40	&120	&500	&6102	\\
%5	&4	&10	&80	&1080	&300	&2884	\\
%5	&12	&20	&60	&120	&300	&360	\\
%		\hline
%3	&4	&10	&40	&600	&400	&4202	\\
%2	&6	&20	&40	&120	&500	&6105	\\
%%4	&4	&14	&80	&1080	&300	&-2923	\\
%5	&12	&14	&60	&600	&300	&1500	\\
%3	&6	&14	&60	&600	&300	&1486	\\
%4	&4	&14	&80	&1080	&300	&2923	\\
%		\hline
%4	&8	&18	&80	&1080	&300	&2971	\\
%3	&8	&10	&50	&1080	&300	&2530	\\
%2	&12	&16	&40	&120	&400	&3077	\\
%2	&10	&18	&60	&1080	&300	&2733	\\
%4	&10	&10	&50	&1080	&300	&2535	\\
%		\hline
%		\hline
%	\end{tabular}
%	\caption{}
%	\label{tab:emma}
%\end{table}
%


\subsection{Fitting via Machine Learning}
\td{scarce data may lead to overfitting\cite{Lecun1995conv}}\\
Python and sci-kit learn \td{cite} was used to implement a linear fit model, and SVR with the kernels polynomial, rbf and sigmoid. 
The space of hyper parameters C, the degree (in case of polynomial), epsilon and gamma was scaned. 

The independent variables are located on a partially irregular grid.  

