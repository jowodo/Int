\section{Data Processing}
\label{sec:eval}
%Reading list
%\begin{itemize}
    %\item https://de.wikipedia.org/wiki/Globaler_F-Test
    %\item https://de.wikipedia.org/wiki/Optimale_Versuchsplanung
    %\item Curse of dimensionality scholar
    %\item linear regression von ungar 
    %\item ridge regression + support vector machine
    %\item correlation vs covariance
    %\item https://en.wikipedia.org/wiki/Regression_analysis
    %\item https://en.wikipedia.org/wiki/Noise_reduction
    %\item https://en.wikipedia.org/wiki/Feature_extraction
    %\item https://en.wikipedia.org/wiki/Feature_selection
    %\item https://en.wikipedia.org/wiki/Dimensionality_reduction
    %\item file:///home/pur/Doc/Uni/Chemie_Master/Int_AIT/Int.git/Code/ML/98_things_can_go_wrong_in_ML_project.html
    %\item https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
    %\item https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA
%    \item 
%\end{itemize}
%\td{describe problem and solution found}
Multiple \gls{iv} curves were measured for every sample.
The main difficulty faced in processing the data was to present the measurements obtained 
from a sample in one representative number. 
%
%\todo{why didn't i just take this?}
%\td{L2 norm because it might be easier to compare to other methods. }
%\td{inspired by euclidean norm/L2 distance from ideal case} 
%\td{wanted to put more weight on bad shunts}
%\td{plot avg, avg(log), avg(log-13)}
%
For every I-V curve (i.e.\,for every aluminium dot contact) the gradient $g$ at $V=0$ (conductance) is calculated by taking 
5 points before and 5 points after the origin with at one point distance from zero (which boils
down to the data points from $\pm$\SI{0.02}{\volt} to $\pm$\SI{0.07}{\volt}), 
calculating averages $\overline{V}$ and $\overline{I}$ of each point cloud and taking the ratio of differences:
%averaging their V and I values and calculating
\begin{equation}
    %g = \frac{I_{+m+d} - I_n}{V_{n+1} - V_n}.
    g = \frac{\overline{I}_{+} - \overline{I}_-}{\overline{V}_{+} - \overline{V}_-}
\end{equation}
%
The average of each conductance would be an easy and naive choice but difficult to represent a 
sample correctly since the possible values of conductance span across several magnitudes.
So the average of base 10 logarithm of conductances is the next nearby \textit{ansatz}.
%
In order to make the metric closer 
%and better comparable to the 
\gls{mse} (and to penalize deviation from ideal more than linearly) the average of the squared difference 
to the ideal non-conduction case ($g=10^{-13}$) was chosen instead of the average of logarithms and shall be called conducticity hereafter for lack of a better term. 
%As a measure of conductance a distance $D$ from an ideal non-conducting case. 
%The average of the negative base 10 logarithm subtracted from an ideal non-conducting gradient of $10^{-13}$ 
%and the mean of all conductances was taken
%
\begin{equation}
    \gamma = \sum_i^N \frac{ (-log_{10}(g_i) - 13)^2}{N}
	\label{eq:gamma}
\end{equation}
Another measure is the density of shorted species $\rho$ (pin hole density), calculated in following way:
\begin{equation}
	s_i = \begin{cases}
        1 &\text{if} \quad -log_{10}(g_i) < 5 \\
        0 &\text{if} \quad -log_{10}(g_i) \geq 5 \\
	\end{cases}
\end{equation}
\begin{equation}
	\rho = \sum_i^N \frac{s_i}{N}
	\label{eq:rho}
\end{equation}
%Other estimates of the conductance are the averages:
%\begin{equation}
%	G_1 = log \left( \sum_i^N \frac{g_i}{N} \right)
%\end{equation}
%
%\begin{equation}
%	G_2 =  \sum_i^N \frac{log(g_i)}{N}
%%\end{equation}

\section{Sample Selection}
\label{sec:ss}
An evolutionary approach was chosen, namely a multi-objective 
\gls{pso} algorithm called \gls{emma}\cite{villanova2010function,Kennedy1995,Breiman1997,Carta2011}.
%
%"PSO is a population based heuristic inspired by the flocking behavior of birds. 
%To simulate the behavior of a swarm, each bird (or particle) is allowed to fly towards the optimum solution."\cite{villanova2010function}
%
Initially, the input parameters (independent variables), their boundaries and number of equidistant levels for each parameter are declared (see table \ref{tab:input}).
Next, the output variables (dependant variables), their weights in the objective function (the function which should be optimized) are specified and if they should be minimized or maximized is noted.
%
%An initial population of particles, i.e. experiments with certain parameters, is chosen out of the population space (space spanned by all possible combinations of input parameters), 
\begin{table}[htb]
	\centering
	\begin{tabular}{cc cc cc}
		\hline\hline
        $c_{zr}$ [\SI{22}{\milli\mol\per\liter}]	&$\lambda$	&$T_{DB}$[\oc{}]	&$v_{DB}$[\mm{}/\s{}]	&$T_{cal}$[\oc{}]	&$v_{cal}$[\oc{}/\h{}]	\\
		\hline
		2				&4		&40					&10				&300				&120	\\
		3				&6		&50					&12				&400				&360	\\
		4				&8		&60					&14				&500				&600	\\
		5				&10		&70					&16				&					&840	\\
						&12		&80					&18				&					&1080	\\
						&		&					&20				&					&		\\
		\hline\hline
	\end{tabular}
	\caption{Discrete levels of each input parameter }
	\label{tab:input}
\end{table}

The \textit{optimizands} where chosen to be $\gamma$, $\rho$, $\lambda$ and $v_{cal}$, 
where $\gamma$, $\rho$ and $\lambda$ are to be minimized and $v_{cal}$ to be maximized.
%which all should be minimized except for $v_{cal}$.
The next step is to generate an initial population (ensemble of experiments), which is chosen randomly from the population space. 
The experiments are executed, measured and evaluated according to section 
\ref{sec:exp} and the responses conducticity $\gamma$ (see eq. \ref{eq:gamma}), pin hole density $\rho_s$ (see eq. \ref{eq:rho}), 
$\lambda$ (numbers of layers) and $v_{cal}$ (heating rate of calcination process in 
\oc{}/\minutes{}) are supplied to the program. 
The program, then, uses this data to estimate a response for each output variable 
for the entire population space
(and to choose a fraction of the initial population which is allowed to propagate).
%The response variables for the entire population space is calculated. 
The current population - each of the particles independently - 
are given a random velocity vector associated to them which is directed to the best predicted sample. 
%moves towards the optimum solution.
The population for the next time step is generated and the experiments are again executed, measured and evaluated.
Each time step thus includes experiment identification, solution preparation, coating deposition and layer analysis. 

\ds{system behavior are discussed in Modeling the Results.
Algorithm and Coating Features. The theoretical efficiency
of the proposed approach has been verified using a simulation
based on a preliminary experimental model (48 recipes evalu-
ated, see the Supporting Information, S3). Then, 15 new experi-
ments were selected for each subsequent time instant. Each time
instant involves experiment identification, solution preparation,
coating deposition, and spot analysis.
}%\cite{Carta2011}

\section{Fitting via Machine Learning}
%\td{scarce data may lead to overfitting\cite{Lecun1995conv}}\\
Python 3.10.9 and sci-kit learn 1.1.2\cite{pedregosa2011scikit} were used to implement a linear fit model, \gls{krr} and \gls{svm} with polynomial, \gls{rbf} and sigmoid kernels. 
%\subsection{Grid Search}
A grid search over a set of hyperparameters was executed. 
%https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py
%https://scikit-learn.org/stable/modules/svm.html#svc
% KRR 
The hyperparameters which were examined in \gls{krr} were kernel, $\gamma_{ml}$ and $\alpha$.
The kernel took the form of \gls{rbf}, sigmoid or polynomial with degree 1--3.
The parameters $\alpha$ and $\gamma_{ml}$ (subscript to distinguish from \textit{optimizand}) took the values 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2
and 0.01, 0.05, 0.1, 0.5, 1.0, 5, 10, respectively.
Note that $\gamma_{ml}$ is used differently in each kernel. 
% SVM
The kernels for \gls{svm} were the same as for \gls{krr}. 
Further hyper parameters were $C$, $\varepsilon$ and $\gamma_{ml}$, 
where $C$ took the values 0.05--0.95 with step size 0.05, $\varepsilon$ 0.02--3.8 with 0.2 steps 
and $\gamma_{ml}$ 0.01, 0.05, 0.1, 0.5, 1, 5, 10, and \texttt{scale}. 
Where \texttt{scale} equals to $\frac{1}{d \cdot \sigma(\mathbf{X})}$ with $d$ being the number of features 
and $\sigma(\mathbf{X})$ the variance of the input variables. 
% GRID SEARCH 
The input variables ($c_{zr}$, $\lambda$, $v_{DB}$, $T_{DB}$, $v_{cal}$, $T_{cal}$) 
of the \gls{emma} data set were scaled before each calculation.
% https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge.score
% https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR.score
For each calculation the 5-fold cross validation score was calculated. 
The score equals to the coefficient of determination $R^2$ which defined as 
$1-\frac{\sum (y_i - \hat{y}_i)}{\sum (y_i - \bar{y}_i)}$.
For each method (\gls{krr} and \gls{svm}) and each \textit{optimizand} ($\gamma$ and $\rho$) 
the parameters with the highest score were noted.
% 
Finally, the different data sets were predicted for each model and \gls{mae} and \gls{mse} were calculated.
These values were also calculated for \gls{emma} and the linear model in order to compare the different methods. 
