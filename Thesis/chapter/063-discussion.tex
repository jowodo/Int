\subsection{Diskussion}
\subsubsection{Mistakes and Solutions}
This chapter is structured as a letter 
which my present self thinks that would have helped my past self 
if recieved before the start of the practical part of my master practikum:\\

to discuss: results from EMMA or rather which the data present are counter intuitive. 
why lowest conductivity with lowest TCal and why is there a minimum for layers? 
low TDOC results in lower conductivity... okay 

\td{"An important concept in inferential statistics is that the amount of information you can learn about a population is limited by the sample size. The more you want to learn, the larger your sample size must be." - by \url{https://blog.minitab.com/en/adventures-in-statistics-2/the-danger-of-overfitting-regression-models} } 
\td{"overfitting a regression model occurs when you attempt to estimate too many parameters from a sample that is too small." - same source}
\td{"However, if the effect size is small or there is high multicollinearity, you may need more observations per term." -same source} 

\iffalse
Dear Johann, 

I know that you (I) won't believe this but I'm you in the future. 
So you are me in the past and I have a few hints on how to avoid some stone in your way 
and how to hopefully be more successful in you master thesis. 
"I make a plan. I break a plan. But I'm never without a plan." - a nice quote which I couln't zuschreiben, but which holds a lot of truth. 
Planning can give you the chance to zoom out and force yourself to get an overview. 
It let's you decide how much time you want to spend on each todo. 

The next tip is that at the beginning of a project the research phase shouldn't be underestimated. 
Take time to dig into the basics and discuss with colleagues if you have the chance. 
Try to understand the methods you'll use as well as possible. 
Most importantly, high light advantages and disadvantages of the methods and competing/alternatives. 
Like in a design project. Lay out as many as diverse solutions as possible, 
compare, and ick the best parts.
The research phase let's you learn from other's mistakes and saves a lot of time. 

Another important task which preceeds/anti-follows above mentioned is to define the objective as clear as possible.  
%\td{ideal objective is to create response surface approximation}\\
It will help to set a clear goal for each todo, e.g. 
the primary goal of a screening experiment is to identify the active factors.\cite{miller2001using}
Which question should be answered and aufstellen a null Hypothesis. 
This is often very trivial, but let's you focus on the correct path. 

The mistake which I regret the most is that I overestimated the vereinfachenden factor of reducing the splits (or discrete points between two extrems, which a variable can take) instead of the reduction of dimensions which eleminates a whole degree of freedom. 
I expected my algorithm of choice to perform magic. 
A measurement to compare different algorithms could be, dimension and ratio of measured sampled to sample space. 
Im vergleich to other algo I used a lot of dimensions and a low ratio.
A low ratio gives rise to a high probability of the data being statistically insignificant, especially if the error is realtively large. 

All the best and stay persistent, \\  
Johann \\
\fi


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Mistakes} 
\td{the input vars were not regularized/normalized, naively like a small child eating old food and regretting not informing oneself earlier}\\
\td{too much data as input, dimensionality reduction (PCA) and optimization}\\
\td{pre optimization was used to check if limits where okay, but could also have been used to sieve out factors without impact on response (see miller 2001 \cite{miller2001using} section 1) }
\td{adding a extra variable to optimize was to optimistic and ich hab mir damit eindeutig 
(im nachhinein) steine in den Weg gelegt.}
The space seems to be too big for the small sample size.
%-
Look at relation of space size and sample size here and in Hu2016.:
\td{In example of documentation 2 input variables and 1-2 responses initial population=10
, subsequent population=5, but 10 iterations. Clearly more input varibles and less 
iterations (as in actual experiments) lead to suboptimal results.}
%
- The exploration-vs-exploitation parameters of the model were also not set accordingly.
see \href{https://search.r-project.org/CRAN/refmans/emma/html/emma.html}{source}
%
- the data has a lot of error, but because the production process takes so long and the 
limited time and the chosen optimisation method, the experiments weren't weiderholt
how much variance is in data? 
%
data-points-to-indep-vars ratio as measurement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Next time better}
{The primary goal of a screening experiment is to identify the active factors.
A secondary goal is to provide a simple model that captures the essential features of the 
relationship between these active factors and the response—that is, to identify the active effects. \cite{miller2001using}}
%
\td{if i would plan the optimisation again, I would take less input variables (fokus on 
conc,vdoc and tdoc or even only vdoc and tdoc) which may or may not have multiple maxima.}
%
- The most time consuming part was definitively the prelaminary studies.
this time could have been verkürzt by testing a wide array of diverse recipes from the literature
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Next steps}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{diskussion and outlook}


- The data is spread across the datenraum, such that it is not trivial to 
(1) find the variable with the most variance and 
(2) to fit in order to understand to impact of noise on the data. 

- Dimension reduction bietet sich stark an bei solcher Datenlage. 

Welche methode funktioniert bei relativ vielen variablen, wenig datenpunkten und Noise. 

Am besten waere feature extraction (variable x1 hat am meisten einfluss) 
ginge auch wenn man bei pca den einfluss von verschiedenen 
I stand in front of this problem where the solution is to not use as many indep vars von vorne herein. 

-\td{for 1F c(Zr) = 44.6 mmol/L
if n is number of samples, and p is number of input variables, then n >> p 
In this case n\~50 and p\~6 which give n/p=8+1/3
in EMMA documentation example n=55and p=1-2, n/p=55-22.5
}
\td{longer literature research (first solution,second solution) and PSO wrongly eingeschaetzt, 
could have been verhindert durch lesen the docu and more papers more thoughoughly. 
Ich war geblendet von dem was ich wollte und dadurch nicht realistisch}

%\td{I-V: 2 terminal measurement one terminal was varibale and the other the ground from $-5*10^{-3}$ V to $5*10^{-3}$ V with steps of $10^{-2}$ measure current from back bone to backbone if shorted, then can measure actual resistance of layer. resistance of steel is neglectable. In order to get an impression of the quality of the layer mutliple contact (picture) are sputtered (throuhg a mask) and statistics: Two angabewerte: the weighted durchschnit and the number of pinholes, ie the number of contacts which were shorted, have an resistance below an threshold. if tunneling, iv follows powerlaw, if direct contact, iv follows linear }

- \td{I wanted to exhaust the possibilities of EA (not having enough data). i should have rather einengen the search raum as much as possible (spreading the search room too much) and then do optimisation: fix layer count to 3 fix conc to 3F and fix calc temp and rate (or only use two extreme values)}
- \td{I wanted to let the algorithm show what I already knew instead of letting my a priori knowledge constrain the model before starting.}
- factorial design (classical \gls{doe} is more robust at feature extraction for error laden data \cite{giunta2003overview}
-\td{"Similarly, it is often wise not to plan a comprehensive experiment that involves a large number of factors of interest. Such an experiment presupposes that most or all of the factors are important contributors to changes in the response variable and that they contribute jointly; i.e., that individual factor effects and their interactions are statistically significant and meaningful." - from Gunst2007}
but on the other hand: If one ain't sure a if factor is relevant, the model should be able to detect if there is a influence. - from Haertler2014?
- let's rule out different input variables: \\
does the heating temperature influence the resistance?  no \\
does the heating velocity influence the resistance?  hardly\\
do conc and layers number influence resistance? linear \\
do db velocity and db temperautre influence resistance? how\\
- somebody very wise (my superviser) once said: we need to able to predict it otherwise it is just an engineering problem \td{(look at mail)}
- "Je grösser nämlich die Anzahl freier Parameter im Modell ist, umso grösser ist der erforderliche Datenumfang und umso ungenauer wir die statistisch gewonnene Aussgae bei gleichem Datenumfang." - Gisela Härtler\cite{haertler2014statistisch}
- \td{The disadvantage of ANOVA is that information is lost because independent variables are assumed categorical even though they are ordinal}
-comparison of population size: 
\verb|see Notes/ga_from_lit_summarj.txt|
what is the minimum of population*generation and what is 10*5 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Learning from Data}
General experimental procedure from Cherkassky and Mulier\cite{cherkassky1998learning}
\begin{enumerate}
	\item State the problem 
	\item Formulate the hypothesis
	\item Design the experiment/generate the data
	\item Collect the data and perform preprocessing
	\item Estimate the model 
	\item Interpret the model/draw the conclusions
\end{enumerate}
\textbf{1. Statement of the Problem} There was no clear statement of the problem. Now I would formulate it: "How to produce most insulating layer with least energy? How do we define most insulating?" That's why there are two output variables rather than one.
\textbf{2. Hypothesis Formulation} formulate an unknown dependency and define input and output variables.
\textbf{3. Data Generation and Experiment Design} can be either in control of the modeler (designed experiment) or in an observational setting. The data collection can affect the sampling distribution and influence the next steps. 
\textbf{4. Collect the Data and Perform Preprocessing} here outliers are detected and data preprocessing/encoding/feature selection. Scaling by standard deviation might be a good idea, but independent scaling of variables can lead to suboptimal representation for the learning task. Feature selection: A small number of informative features make the task of estimating dependencies easier. 
\textbf{5. Model Estimation} The main goal is to construct a model for accurate prediction of future outputs
\textbf{6. Interpret the Model and Drawing Conclusions} The interpretability and accuracy of the model are compete. In classical statistics such as linearly parametrized function will suit both requirements. More complex and flexible models might lead to better estimates with less interpretability. Identifying the most important input variables. \cite{cherkassky1998learning}

There are not only \textbf{x}(input) and y(output) variables, but also \textbf{z}(uncontrolled/unobserved input variables).
Issues of methods for learning from data: 
\begin{itemize}
	\item How to incorporate a priori assumptions into learning? 
	\item How to measure model complexity (i.e. flexibility to fit the training data)?
	\item How to find a optimal balance between the data and a priori knowledge? 
\end{itemize}
\cite{cherkassky1998learning}

"Learning is the process of estimating an unkown (input,output) dependency of structure of a system using a limit number of observations."\cite{cherkassky1998learning}

The process informal part of selection of input and output variables, data encoding/representation and incorporating a prior knowledge into the design of the learning system is often more critical for an overall success than the design of the learning machine itself.\cite{cherkassky1998learning} (page 25)

"Do not attempt to solve a specified problem by indirectly solving a harder general problem as an intermediate step."\cite{cherkassky1998learning}(p33)

"This approach works well only when the number of training samples is large relative to the (prespecified) model complexity (or the number of free parameters)".\cite{cherkassky1998learning}(p41)

Modeling bias in statistics is the discrepancy of the mismatch between parametric assumptions and the true dependency.
Modeling bias is overcome by using very flexible approximation function, with the tradeoff of more complex inductive (learning) steps. 

Any learning process requires the following(p40): 
\begin{itemize}
	\item A (wide, flexible) set of approximating functions $\mathbf{f}(\mathbf{x},\omega),\; \omega \in \Omega$
	\item A priori knowledge to impose contrains
	\item An inductive principle (what needs to be done)
		\begin{itemize}
			\item penalizeation (regularization) inductive principle
			\item early stopping rules (with ANN, difficult to control and interpret)
			\item structural risk minimization (SRM) (order according to complexity)
			\item Bayesian Inference (add subjectivity to learning machine)
			\item Minimum Description Length (MDL, cryptic complicated uninteresting)
		\end{itemize}
	\item A learning method (how does it need to be done,implementation)
\end{itemize}
\cite{cherkassky1998learning}

\textbf{Curse and Complexity of Dimensionality}
goal is to estimate function with finite samples, so it's always inaccurate (biased). 
meaning full estimate only possible with high sampling density, which is difficult for high dimensional functions.
\begin{enumerate}
	\item Sample sizes yielding the same density increse exponentially with dimension.
	\item A large radius is needed to enclose a fraction of the data points in a high-dimensional space.
	\item Almost every point is closer to an edge than to another point.
	\item Almost every point  is an outlier in its own projection.
\end{enumerate}
\textbf{Other statistics}
\begin{itemize}
    \item f-test (include to ANOVA \url{https://quantifyinghealth.com/f-statistic-in-linear-regression/})
    \item p-value: Is the probability that a data point is observed under the null hypothesis. A small value argues against the null hypothesis. 
    \item t-test: will tell you if a variable is statistically significant. 
    \item f-test: will tell you if a group of variables is jointly significant
    \item T-test (signal-noise-ratio \href{https://statisticsbyjim.com/hypothesis-testing/t-tests-1-sample-2-sample-paired-t-tests/}{(click here)}): if variable are stat. significant; 
    \item F-test (ratio of variances \href{https://statisticsbyjim.com/anova/f-tests-anova/}{click here}): if group of vars stat. sig \url{https://quantifyinghealth.com/f-statistic-in-linear-regression/}
\end{itemize}

am wichtigsten is das Ziel der untersuchung! \cite{haertler2014statistisch}
"ist das modell fraglich, sollte man die Daten so erheben, als gälte das nächst kompliziertere Modell" \cite{haertler2014statistisch}
Er könnte nun die Unterschiede zwischen den Schulen "statisteisch messen"un den Schuleinfluss in der abschliessenden Analyse "herausrechnen". \cite{haertler2014statistisch}
Eine gefährliche Fehlerquelle ist dei Erwartung eines bestimmten Ergebnisses durch den Experimentator. \cite{haertler2014statistisch}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{LINKS}
\begin{itemize}
    \item stat vs ML \url{https://medium.com/source-institute/ai-vs-statistics-c2485f9df126} and 
    \item https://towardsdatascience.com/no-machine-learning-is-not-just-glorified-statistics-26d3952234e3?gi=3f94b919de45
    \item https://towardsdatascience.com/are-you-aware-how-difficult-your-regression-problem-is-b7dae830652b calculate error/smoothness etc
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Would be easier to fit with single factor at a time variation or latin hyper cuber? 
%with same search-space to actual-data ratio
%- Would it also be easier for MARS or ML to find fitting function?
%plot predictions from EMMA and ML. or rather compare MAE

\td{what are the reasons for large error?} 
\begin{itemize} 
    \item ageing of solution 
    \item varying humidity
    \item varying room temperature 
    \item impurities in solutions
    \item inhomogeneity of film 
    \item measuring error
    \item 
\end{itemize}

What is the reason for aging (O2,H2O?, not sealing accelerates) ? 
Why does \gls{ipo} decelerate the aging process? ( ipo fits better to solvent(buoh)?)
As the original solvent was 2-methoxy-ethonanol instead of \ch{buoh}. 

\href{https://pubs.acs.org/doi/10.1021/ci0342472}{overfitting}
\td{COMMENT: solution age was not accounted for}

\td{two main problems: 1. too many input variables; 2. too many output varibales; at2: as some input variables were also output variables, MARS couldnt find correct BFs. This could be used though: use input var also as output var if you think they are inportant, this way they will be more likely to be choosen in basis function}

\td{The goal of introducing \gls{ml} to this project was to apply and delve/vertiefen into subject which I've been studying during my undergrad courses. This seemed like the perfect opputrunity. And indeed I learned a lot. Even if only although \gls{ml} is a hot topic at the moment, it can not solve every problem. Two important things have I learned regarding \gls{ml}: 
1.) The methods bringen sich nix if one doesn't know how to apply them which directly leads into my second erkenntnis
2.) If one wants to use a tool, one should know how to use it and its innerlichkeiten in order to use it well.
The second point is not the best comparison because one doesn't need to know how a light switch works in order to use it effectively use it.} 
