\subsection{Diskussion}
\td{ideal objective is to create response surface approximation}\\
\td{the input vars were not regularized/normalized, naively like a small child eating old food and regretting not informing oneself earlier}\\
\td{too much data as input, dimensionality reduction (PCA) and optimization}\\
\td{pre optimization was used to check if limits where okay, but could also have been used to sieve out factors without impact on response (see miller 2001 \cite{miller2001using} section 1) }
{The primary goal of a screening experiment is to identify the active factors.
A secondary goal is to provide a simple model that captures the essential features of the 
relationship between these active factors and the response—that is, to identify the active effects. \cite{miller2001using}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{diskussion and outlook}
\td{if i would plan the optimisation again, I would take less input variables (fokus on 
conc,vdoc and tdoc or even only vdoc and tdoc) which may or may not have multiple maxima.}
\td{adding a extra variable to optimize was to optimistic and ich hab mir damit eindeutig 
(im nachhinein) steine in den Weg gelegt.}
The space seems to be too big for the small sample size.
Look at relation of space size and sample size here and in Hu2016.
\td{In example of documentation 2 input variables and 1-2 responses initial population=10
, subsequent population=5, but 10 iterations. Clearly more input varibles and less 
iterations (as in actual experiments) lead to suboptimal results.
When plotting input vars  against the response variables, no trend
- The exploration-vs-exploitation parameters of the model were also not set accordingly.}
see \href{https://search.r-project.org/CRAN/refmans/emma/html/emma.html}{source}
Would be easier to fit with single factor at a time variation or latin hyper cuber? 
with same search-space to actual-data ratio
- Would it also be easier for MARS or ML to find fitting function?
plot predictions from EMMA and ML. or rather compare MAE
- the data has a lot of error, but because the production process takes so long and the 
limited time and the chosen optimisation method, the experiments weren't weiderholt
how much variance is in data? 
- The most time consuming part was definitively the prelaminary studies.
this time could have been verkürzt by testing a wide array of diverse recipes from the literature
- The data is spread across the datenraum, such that it is not trivial to 
(1) find the variable with the most variance and 
(2) to fit in order to understand to impact of noise on the data. 
- Dimension reduction bietet sich stark an bei solcher Datenlage. 
Welche methode funktioniert bei relativ vielen variablen, wenig datenpunkten und Noise. 
Am besten waere feature extraction (variable x1 hat am meisten einfluss) 
ginge auch wenn man bei pca den einfluss von verschiedenen 
I stand in front of this problem where the solution is to not use as many indep vars von vorne herein. 
- EMMA was finished on 23.02.2021 (make timeline?)
- TODO: check which software versions
-\td{for 1F c(Zr) = 44.6 mmol/L
if n is number of samples, and p is number of input variables, then n >> p 
In this case n\~50 and p\~6 which give n/p=8+1/3
in EMMA documentation example n=55and p=1-2, n/p=55-22.5
plot x vs G and hold all other vars const
write script to do that}
\todo{make sample time line with ipo vs acoh, al vs ag, hg vs doc, ...}
\td{longer literature research (first solution,second solution) and PSO wrongly eingeschaetzt, 
could have been verhindert durch lesen the docu and more papers more thoughoughly. 
Ich war geblendet von dem was ich wollte und dadurch nicht realistisch}
%\td{I-V: 2 terminal measurement one terminal was varibale and the other the ground from $-5*10^{-3}$ V to $5*10^{-3}$ V with steps of $10^{-2}$ measure current from back bone to backbone if shorted, then can measure actual resistance of layer. resistance of steel is neglectable. In order to get an impression of the quality of the layer mutliple contact (picture) are sputtered (throuhg a mask) and statistics: Two angabewerte: the weighted durchschnit and the number of pinholes, ie the number of contacts which were shorted, have an resistance below an threshold. if tunneling, iv follows powerlaw, if direct contact, iv follows linear }
- \td{I wanted to exhaust the possibilities of EA (not having enough data). i should have rather einengen the search raum as much as possible (spreading the search room too much) and then do optimisation: fix layer count to 3 fix conc to 3F and fix calc temp and rate (or only use two extreme values)}
- \td{I wanted to let the algorithm show what I already knew instead of letting my a priori knowledge constrain the model before starting.}
- factorial design (classical \gls{doe} is more robust at feature extraction for error laden data \cite{giunta2003overview}
-\td{"Similarly, it is often wise not to plan a comprehensive experiment that involves a large number of factors of interest. Such an experiment presupposes that most or all of the factors are important contributors to changes in the response variable and that they contribute jointly; i.e., that individual factor effects and their interactions are statistically significant and meaningful." - from Gunst2007}
but on the other hand: If one ain't sure a if factor is relevant, the model should be able to detect if there is a influence. - from Haertler2014?
- let's rule out different input variables: \\
does the heating temperature influence the resistance?  no \\
does the heating velocity influence the resistance?  hardly\\
do conc and layers number influence resistance? linear \\
do db velocity and db temperautre influence resistance? how\\
- somebody very wise (my superviser) once said: we need to able to predict it otherwise it is just an engineering problem \td{(look at mail)}
- "Je grösser nämlich die Anzahl freier Parameter im Modell ist, umso grösser ist der erforderliche Datenumfang und umso ungenauer wir die statistisch gewonnene Aussgae bei gleichem Datenumfang." - Gisela Härtler\cite{haertler2014statistisch}
- \td{The disadvantage of ANOVA is that information is lost because independent variables are assumed categorical even though they are ordinal}
-comparison of population size: 
\verb|see Notes/ga_from_lit_summarj.txt|
what is the minimum of population*generation and what is 10*5 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Learning from Data}
General experimental procedure from Cherkassky and Mulier\cite{cherkassky1998learning}
\begin{enumerate}
	\item State the problem 
	\item Formulate the hypothesis
	\item Design the experiment/generate the data
	\item Collect the data and perform preprocessing
	\item Estimate the model 
	\item Interpret the model/draw the conclusions
\end{enumerate}
\textbf{1. Statement of the Problem} There was no clear statement of the problem. Now I would formulate it: "How to produce most insulating layer with least energy? How do we define most insulating?" That's why there are two output variables rather than one.
\textbf{2. Hypothesis Formulation} formulate an unknown dependency and define input and output variables.
\textbf{3. Data Generation and Experiment Design} can be either in control of the modeler (designed experiment) or in an observational setting. The data collection can affect the sampling distribution and influence the next steps. 
\textbf{4. Collect the Data and Perform Preprocessing} here outliers are detected and data preprocessing/encoding/feature selection. Scaling by standard deviation might be a good idea, but independent scaling of variables can lead to suboptimal representation for the learning task. Feature selection: A small number of informative features make the task of estimating dependencies easier. 
\textbf{5. Model Estimation} The main goal is to construct a model for accurate prediction of future outputs
\textbf{6. Interpret the Model and Drawing Conclusions} The interpretability and accuracy of the model are compete. In classical statistics such as linearly parametrized function will suit both requirements. More complex and flexible models might lead to better estimates with less interpretability. Identifying the most important input variables. \cite{cherkassky1998learning}

There are not only \textbf{x}(input) and y(output) variables, but also \textbf{z}(uncontrolled/unobserved input variables).
Issues of methods for learning from data: 
\begin{itemize}
	\item How to incorporate a priori assumptions into learning? 
	\item How to measure model complexity (i.e. flexibility to fit the training data)?
	\item How to find a optimal balance between the data and a priori knowledge? 
\end{itemize}
\cite{cherkassky1998learning}

"Learning is the process of estimating an unkown (input,output) dependency of structure of a system using a limit number of observations."\cite{cherkassky1998learning}

The process informal part of selection of input and output variables, data encoding/representation and incorporating a prior knowledge into the design of the learning system is often more critical for an overall success than the design of the learning machine itself.\cite{cherkassky1998learning} (page 25)

"Do not attempt to solve a specified problem by indirectly solving a harder general problem as an intermediate step."\cite{cherkassky1998learning}(p33)

"This approach works well only when the number of training samples is large relative to the (prespecified) model complexity (or the number of free parameters)".\cite{cherkassky1998learning}(p41)

Modeling bias in statistics is the discrepancy of the mismatch between parametric assumptions and the true dependency.
Modeling bias is overcome by using very flexible approximation function, with the tradeoff of more complex inductive (learning) steps. 

Any learning process requires the following(p40): 
\begin{itemize}
	\item A (wide, flexible) set of approximating functions $\mathbf{f}(\mathbf{x},\omega),\; \omega \in \Omega$
	\item A priori knowledge to impose contrains
	\item An inductive principle (what needs to be done)
		\begin{itemize}
			\item penalizeation (regularization) inductive principle
			\item early stopping rules (with ANN, difficult to control and interpret)
			\item structural risk minimization (SRM) (order according to complexity)
			\item Bayesian Inference (add subjectivity to learning machine)
			\item Minimum Description Length (MDL, cryptic complicated uninteresting)
		\end{itemize}
	\item A learning method (how does it need to be done,implementation)
\end{itemize}
\cite{cherkassky1998learning}

\textbf{Curse and Complexity of Dimensionality}
goal is to estimate function with finite samples, so it's always inaccurate (biased). 
meaning full estimate only possible with high sampling density, which is difficult for high dimensional functions.
\begin{enumerate}
	\item Sample sizes yielding the same density increse exponentially with dimension.
	\item A large radius is needed to enclose a fraction of the data points in a high-dimensional space.
	\item Almost every point is closer to an edge than to another point.
	\item Almost every point  is an outlier in its own projection.
\end{enumerate}
\textbf{Other statistics}
\begin{itemize}
    \item f-test (include to ANOVA \url{https://quantifyinghealth.com/f-statistic-in-linear-regression/})
    \item p-value: Is the probability that a data point is observed under the null hypothesis. A small value argues against the null hypothesis. 
    \item t-test: will tell you if a variable is statistically significant. 
    \item f-test: will tell you if a group of variables is jointly significant
    \item T-test (signal-noise-ratio \href{https://statisticsbyjim.com/hypothesis-testing/t-tests-1-sample-2-sample-paired-t-tests/}{(click here)}): if variable are stat. significant; 
    \item F-test (ratio of variances \href{https://statisticsbyjim.com/anova/f-tests-anova/}{click here}): if group of vars stat. sig \url{https://quantifyinghealth.com/f-statistic-in-linear-regression/}
\end{itemize}

am wichtigsten is das Ziel der untersuchung! \cite{haertler2014statistisch}
"ist das modell fraglich, sollte man die Daten so erheben, als gälte das nächst kompliziertere Modell" \cite{haertler2014statistisch}
Er könnte nun die Unterschiede zwischen den Schulen "statisteisch messen"un den Schuleinfluss in der abschliessenden Analyse "herausrechnen". \cite{haertler2014statistisch}
Eine gefährliche Fehlerquelle ist dei Erwartung eines bestimmten Ergebnisses durch den Experimentator. \cite{haertler2014statistisch}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{LINKS}
\begin{itemize}
    \item stat vs ML \url{https://medium.com/source-institute/ai-vs-statistics-c2485f9df126} and 
    \item https://towardsdatascience.com/no-machine-learning-is-not-just-glorified-statistics-26d3952234e3?gi=3f94b919de45
    \item https://towardsdatascience.com/are-you-aware-how-difficult-your-regression-problem-is-b7dae830652b calculate error/smoothness etc
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
